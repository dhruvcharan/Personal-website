# My Thoughts on the Emergence of AI Tools in Knowledge Work

I have been having discussions about the emergence of AI tools with a lot of my colleagues on how it has impacted their productivity, their relationship with a lot of their work and also what it has done for their quality of life as a whole. Now mind you I am a 25yo grad student with most of my points of reference being people in a similar stage of their careers as mine, however, that is in fact the demographic I want to focus on since they are the segment most inordinately affected by this paradigm shift, and I have some thoughts that I wanted to put to paper that convey how I feel about this tidal wave.

I most certainly do not intend to come off as a Luddite when I outline the pitfalls of the overreliance on these systems, that has plagued entry-level knowledge work in these recent months, and I do understand the immense value they bring in democratizing knowledge by making all the information from everywhere all at once accessible to anyone with an internet connection. They are perhaps the most effective systems humans have ever built in terms of transforming unstructured data into coherent meaningful blocks of knowledge. And I will not be discussing the environmental impact despite it being a major conversation that needs to be had. I do in fact concede that in the right hands and used effectively they are a very powerful tool that can supercharge productivity, however, in this writeup, I want to outline why I think people who are at the fringes or in the early stages of knowledge-work space are not the users that at the large benefit from these tools.

The main use of these autoregressive models ( and that adjective is a crucial component for understanding their powers and also their limitations) is to eliminate busywork, however, many have glossed over the BUSY in busywork and have instead resorted to using these models like a hand cranked projector, cranking away at the machine as a substitute for every task that requires some level of thought put into it, that can be writing an email, or solving an assignment, or building a web crawler. The issue with this modality of operating is we are bypassing the building of the neural pathways that are associated with the act of making an earnest attempt. I believe a large majority of users of these systems have not built the calluses from having solved those cognitively demanding tasks enough times earlier. And the issue is we are retraining our instincts to default to looking things up in this unorganized yellow paper of knowledge.

As a detour, I have been a massive fan of old LucasArts adventure games, in particular the Monkey Island Series of Games, it's a genre of these long-winded narratively rich series of puzzles, which are very intricately tied together with the story, and the thrill of the chase is derived from solving these at times hair-pullingly frustrating puzzles. I remember I would get stuck on a certain puzzle and then just give up entirely. This was before I discovered online walkthroughs, and now getting through these games was a breeze, but somewhere I wasn't scratching that itch of solving this by running into walls. I was just open-sesame-ing my way through my problems and that was not as satisfying. It was only after I resolved to treat these walkthroughs as a refuge of last resort did I get my joy of adventure games back, and I haven’t looked back since.

All of this is to say that there is a very large cost to these quick fixes that we building a knowledge base of the cues and nuances that working on a bigger problem often entails, but simply pulling a lever on a slot machine in the hopes it spits out the answer we are looking for(which to its credit it does at a strikingly high rate). The problem is leaning on this tends to leave us with a very partial understanding, we are not so much learning as we are spectating. This is directly in discord with the way we develop our understanding. Human understanding in my experience is gradual, it is akin to stacking ideas on top of one another, with a very sturdy foundation at the base as a crucial component. It's like building a Lego skyscraper, and it needs a sharp singular focus. The way LLMs understand from a very simplified architectural point of view can be understood as solving a very large Jigsaw puzzle, with the model learning to piece ideas (represented by vectors in a nebulous multidimensional space) together. They figure out what goes where by seeing a large number of reference examples and understanding the relationships between words. But when we substitute our learning with that of the models, we miss the growing pains of figuring things out ourselves, and even though we do end up learning something, we often end up with our Lego Tower looking more like a Jenga Tower with very serious structural gaps, very often at the very base of the structure.

I for instance at a certain point where I found myself inundated with work on my plate and assignments found myself resorting to these tools as a sort of instant solution whenever I found myself being stuck, and it did yield results in terms of getting things done, but the unfortunate side effect was I found myself resorting to them at the first sign of inconvenience. It was very much like the walkthrough dependence. This led to what can only be termed as an understanding chasm, wherein I cannot be very confident in saying I learned at the rate I would have had I dug in and persevered through the bugs and frustrations. I ended up having to re-reference things that I should have worked through all the time, which I argue wouldnt have happened had I been more thorough with my work and just like with the walkthroughs I made a commitment to only turn to ol'reliable GPT when I can justify it.

I also have a very ( maybe ill-founded and dramatic ) vision of these tools completely overhauling how knowledge work is conducted as a whole. The first major touchstone in productivity was the assembly line, it was the biggest paradigm shift towards quantifying individual throughput, where humans were merely steps in this largely sequential, automated conveyor belt, tasked with feeding inputs to a machine that cranks out products. In this system, the bottleneck was the human step but the responsibility of producing largely lay with the machine. As we moved to the white-collar model of operating for knowledge work, the roles were somewhat reversed, with humans in charge of doing the things that mattered. The concern is the emergence of these tools over the course of the coming decades could be the domino that ends up upending this modality of work to where a small fraction of the original workforce ends up having to act as the bottlenecks tasked with feeding a machine with data that it churns out results from.

The other concern I have with overreliance on these models is the sameness problem. With the growing degree of usage of these tools, eagle-eyed users can spot recurrent patterns among text written by AI tools, with usage of words like delve, underscores, showcase etc, have seen a very significant and aberrant growth in their usage. [1] These patterns are likely to not be confined to written outputs etc. It is almost impossible to not expect these patterns to not be prominent in the code outputs etc generated by these systems.In fact, this problem can be observed not just within the outputs of LLMs, but any large-scale system governed by AI models. We see this in the homogeneity of all of our algorithmic feeds, or recommendation systems, wherein users are slowly but assuredly being converged into this algorithmic median. This stems not from a specific design choice but rather from the sheer inevitability of the Gaussian curve as the scale of the data these tools ingest grows by leaps and bounds. Kyle Chayka performs an excellent deep dive into this phenomenon of flattening and homogenization in his book Filterworld which I cannot recommend enough[2].

But as an unintended consequence of this, we might find ourselves in a “snake eating it's tail” situation with the feedback loop of overtly similar results from these tools creating the data that trains future versions of these tools. This same-ishness might lead to missing out on small but largely ignored optimizations, bad but prevalent design patterns becoming entrenched in the core of these models, and a compounding of similar serious problems, stemming from a long backlog of legacy code that is a byproduct of the tech maxim of Moving Fast and Breaking Things. This is largely a good thing in terms of innovating quickly and then solving the problem later, but when this same code ends up being the majority of the reference for these models we might find ourselves in a place where this lethargy of not having refactored might come to bite us back. However, with the rapid scale of innovation, I doubt these companies can afford to wait and refactor bad code before they train their models so I suspect these issues might end up persisting.

All of this is to say that with the enormous promise these tools show, we should be just as mindful of the very real pitfalls that we might not stop to reconsider as we blaze this trail into a new frontier of work. I have a lot more thoughts I wanted to get into but this has already dragged on for far too long, so I will end it with this. As we stand to gain a lot but also simultaneously lose some things just as crucial if we are not vigilant, I strongly believe, we must as a society invest just as much time in analog modes of learning, where we solve problems, not by resorting to easy lookups, but by rolling our sleeves, digging in, by reading and doing, by thinking, by trying, and most importantly by failing.